{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wqxOPX6Tjtsa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.2 in c:\\users\\nvgkv\\anaconda3\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: py4j==0.10.9.2 in c:\\users\\nvgkv\\anaconda3\\lib\\site-packages (from pyspark==3.2) (0.10.9.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark==3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IGZBvTqkka3X"
   },
   "outputs": [],
   "source": [
    "import pyspark as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YyGllGqXp51v"
   },
   "outputs": [],
   "source": [
    "file=\"C:/Users/nvgkv/Downloads/BBC News Train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-budW_AVuo5r"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2660/127926885.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#sc.stop()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'spark.executor.memory'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'16g'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'spark.driver.memory'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'16g'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "#sc.stop()\n",
    "conf = ps.SparkConf().setAll([('spark.executor.memory', '16g'), ('spark.driver.memory', '16g')])\n",
    "sc = ps.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2sJqX7Wy-Qkn"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sql_context = SQLContext(sc)\n",
    "voxDf = sql_context.read.format('com.databricks.spark.csv').option('header','true').option(\"delimiter\", \",\").load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BB2sJyxJldZX"
   },
   "outputs": [],
   "source": [
    "prepprocesDF=voxDf.drop(\"title\",\"author\",\"published_date\",\"updated_on\",\"slug\",\"blurb\",\"ArticleId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDJ8tAjuyLHz"
   },
   "outputs": [],
   "source": [
    "voxDf.show(5)\n",
    "prepprocesDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E99l8hXczN20"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "prepprocesDF = prepprocesDF.withColumn(\"text\", regexp_replace(\"Text\", \"<[^>]+>\", \"\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_j_l2Ly09cL"
   },
   "outputs": [],
   "source": [
    "prepprocesDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xlwOxAAM3H2E"
   },
   "outputs": [],
   "source": [
    "prepprocesDF=prepprocesDF.drop(\"body\")\n",
    "prepprocesDF=prepprocesDF.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DjTv60ccANy4"
   },
   "outputs": [],
   "source": [
    "trainDF,testDF=prepprocesDF.randomSplit([0.75, 0.25], seed=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cQQGQBax5pX5"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StringIndexer,CountVectorizer,RegexTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "efI7ZJvZ9zFQ"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram, StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vK48VXkY5tvt"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "#tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "# regular expression tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# Define NGram transformer\n",
    "\n",
    "stopwordremover=StopWordsRemover(inputCol=\"words\", outputCol=\"wordsWithStopwordsfree\")\n",
    "#ngram = NGram(n=2, inputCol=\"wordsWithStopwordsfree\", outputCol=\"bigrams\")\n",
    "# bag of words count\n",
    "countVectors = CountVectorizer(inputCol=\"wordsWithStopwordsfree\", outputCol=\"cf\", vocabSize=20000, minDF=5)\n",
    "#hashtf = HashingTF(numFeatures=2 ** 16, inputCol=\"wordsWithStopwordsfree\", outputCol=\"cf\")\n",
    "idf = IDF(inputCol=\"cf\", outputCol=\"features\", minDocFreq=5)\n",
    "label_string_idx = StringIndexer(inputCol=\"Category\", outputCol=\"label\").setHandleInvalid(\"skip\")\n",
    "pipeline = Pipeline(stages=[regexTokenizer,stopwordremover,countVectors,idf,label_string_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HrkaG-vGZrI6"
   },
   "outputs": [],
   "source": [
    "prepprocesDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SctrLbriVwCV"
   },
   "outputs": [],
   "source": [
    "pipeline_fit = pipeline.fit(prepprocesDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6PJTjA3Fjrd"
   },
   "outputs": [],
   "source": [
    "pipeline_fit = pipeline.fit(prepprocesDF)\n",
    "#pipeline_fit.save(\"pipleline_model_path\")\n",
    "train_df = pipeline_fit.transform(trainDF)\n",
    "test_df = pipeline_fit.transform(testDF)\n",
    "#prepprocesDF = pipeline_fit.transform(prepprocesDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9BOd8wdFz7H8"
   },
   "outputs": [],
   "source": [
    "test_df.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GY04hZ9pIfqv"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, LogisticRegressionModel,NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "#lr =  LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lr=NaiveBayes(smoothing=1)\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6pjxggq2Brb"
   },
   "outputs": [],
   "source": [
    "predictions = lr_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DGMCR1qHIEz5"
   },
   "outputs": [],
   "source": [
    "predictions.show(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esMlQ9uKXtSj"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T49ZUrGRfPr5"
   },
   "outputs": [],
   "source": [
    "#tweeter data pipe line\n",
    "regexTokenizertw = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# Define NGram transformer\n",
    "\n",
    "stopwordremovertw=StopWordsRemover(inputCol=\"words\", outputCol=\"wordsWithStopwordsfree\")\n",
    "#ngram = NGram(n=2, inputCol=\"wordsWithStopwordsfree\", outputCol=\"bigrams\")\n",
    "# bag of words count\n",
    "countVectorstw = CountVectorizer(inputCol=\"wordsWithStopwordsfree\", outputCol=\"cf\", vocabSize=20000, minDF=5)\n",
    "#hashtf = HashingTF(numFeatures=2 ** 16, inputCol=\"wordsWithStopwordsfree\", outputCol=\"tf\")\n",
    "idftw = IDF(inputCol=\"cf\", outputCol=\"features\", minDocFreq=5)\n",
    "#label_string_idx = StringIndexer(inputCol=\"Category\", outputCol=\"label\").setHandleInvalid(\"skip\")\n",
    "pipelinetw = Pipeline(stages=[regexTokenizertw,stopwordremovertw,countVectorstw,idftw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IzGKzkOjds6p"
   },
   "outputs": [],
   "source": [
    "tweets1 = sql_context.read.format('com.databricks.spark.csv').option('header','false').option(\"delimiter\", \",\").load('/content/gdrive/MyDrive/Colab Notebooks/BigDataProject/CategoryPredictModelandData/tuesday.csv')\n",
    "tweets2 = sql_context.read.format('com.databricks.spark.csv').option('header','false').option(\"delimiter\", \",\").load('/content/gdrive/MyDrive/Colab Notebooks/BigDataProject/CategoryPredictModelandData/tuesday1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjSt_y94iob8"
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "  \n",
    "def unionAll(dfs):\n",
    "    return functools.reduce(lambda df1, df2: df1.union(df2.select(df1.columns)), dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4A7yEoGMg5xS"
   },
   "outputs": [],
   "source": [
    "tweets = unionAll([tweets1, tweets2])\n",
    "print(tweets.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CTiwSusNXUl4"
   },
   "outputs": [],
   "source": [
    "tweets = tweets.where(tweets._c0!='null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MTcCAwqXEMSm"
   },
   "outputs": [],
   "source": [
    "print(tweets.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8BiBefB7_QPD"
   },
   "outputs": [],
   "source": [
    "tweets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nS6mbYnFO9NM"
   },
   "outputs": [],
   "source": [
    "tweets=tweets.selectExpr(\"_c0 as text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mK3PJwyNjBm1"
   },
   "outputs": [],
   "source": [
    "pipeline_fittw = pipelinetw.fit(prepprocesDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0tcDY1LjR5D"
   },
   "outputs": [],
   "source": [
    "test_df = pipeline_fittw.transform(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCJfIFNWjcV7"
   },
   "outputs": [],
   "source": [
    "test_df.show(600,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8eKRNcnOUoHk"
   },
   "outputs": [],
   "source": [
    "tweets=pipeline_fittw.transform(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVXP_bPoWDfu"
   },
   "outputs": [],
   "source": [
    "tweets.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QsD7TiDOWM2N"
   },
   "outputs": [],
   "source": [
    "tweetprediction = lr_model.transform(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zT_QcARcWVmd"
   },
   "outputs": [],
   "source": [
    "tweetprediction.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2htfnO0l3WO"
   },
   "outputs": [],
   "source": [
    "tweetprediction=tweetprediction.drop(\"words\",\"wordsWithStopwordsfree\",\"cf\",\"features\",\"rawPrediction\",\"probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_3X-ntP7ormr"
   },
   "outputs": [],
   "source": [
    "tweetprediction.show(n=60,truncate=False,vertical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HguwQvXmrJUa"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,when\n",
    "tweetprediction=tweetprediction.withColumn(\"prediction\", when(col(\"prediction\") == 0.0, \"sport\").when(col(\"prediction\") ==1.0, \"business\").when(col(\"prediction\") ==2.0, \"political\").when(col(\"prediction\") ==3.0, \"entertainment\").otherwise(\"tech\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FS7wno2pGAt1"
   },
   "outputs": [],
   "source": [
    "tweetprediction.show(n=60,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H34sqorAEZNV"
   },
   "outputs": [],
   "source": [
    "tweetprediction.write.csv(\"/content/gdrive/MyDrive/Colab Notebooks/BigDataProject/tuesdaycategoryPredictedDataset\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Preprocessin_and_model_building_voxnews_dataset.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
